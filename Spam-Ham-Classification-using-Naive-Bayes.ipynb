{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AE1AaRcLmksk"
   },
   "source": [
    "# **References:**\n",
    "\n",
    "1. https://www.kaggle.com/dilip990/spam-ham-detection-using-naive-bayes-classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NZbdG7W0j6qt"
   },
   "source": [
    "# **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "ykBXxj1EkHXh",
    "outputId": "4d1bf8b3-226e-4d5f-ee46-59905894586c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import string\n",
    "import heapq \n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qIqPC2cbxJVr"
   },
   "source": [
    "# **Data Preprocessing**\n",
    "\n",
    "1. Removing Stop-words\n",
    "2. Removing Punctuations\n",
    "3. Removing trailing expressions such as \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "vbNkJSGokS1O",
    "outputId": "2428aae9-9995-4ab6-ec5b-b5e1a9b07744"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels                                            message  length\n",
       "0    ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1    ham                      Ok lar... Joking wif u oni...      29\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3    ham  U dun say so early hor... U c already then say...      49\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 194,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message=pd.read_csv('SMSSpamCollection.csv',sep='\\t',names=[\"labels\",\"message\"])\n",
    "message['length']=message['message'].apply(len)\n",
    "message.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UqPUQ9QXkkZV"
   },
   "outputs": [],
   "source": [
    "def preprocessing_text(mess):\n",
    "    nopunc =[char for char in mess if char not in string.punctuation]\n",
    "    nopunc=''.join(nopunc)\n",
    "    ret = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "    if(len(ret) == 0):\n",
    "      return np.nan\n",
    "    if(len(ret) != 0):\n",
    "      return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QQsg_3zokktc"
   },
   "outputs": [],
   "source": [
    "message[\"message\"] = message['message'].apply(preprocessing_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8o2IwZQIJUwU"
   },
   "outputs": [],
   "source": [
    "message = message.dropna()\n",
    "\n",
    "def remove_dot_dot_dot(x):\n",
    "  for i in range(len(x)):\n",
    "    nodot =[char for char in x[i] if char != \"...\"]\n",
    "    x[i] = ''.join(nodot)\n",
    "  return x\n",
    "\n",
    "message[\"message\"] = message[\"message\"].apply(remove_dot_dot_dot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FEPGgs-CkEmS"
   },
   "source": [
    "# **Generating Vocabulary From Available Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nx9btS9dKvoV"
   },
   "outputs": [],
   "source": [
    "dictionary = set()\n",
    "list_of_words = np.array([])\n",
    "for i in range(message[\"message\"].to_numpy().shape[0]):\n",
    "  if(list_of_words.size == 0):\n",
    "    list_of_words = np.array(message[\"message\"].to_numpy()[i])\n",
    "  else:\n",
    "    list_of_words = np.append(list_of_words,message[\"message\"].to_numpy()[i])\n",
    "  for w in (message[\"message\"].to_numpy()[i]):\n",
    "    dictionary.add(w)\n",
    "\n",
    "dictionary = list(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RHBgZjf3ksa-"
   },
   "source": [
    "## **Preparing a look up table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "032NSy-ULEL7"
   },
   "outputs": [],
   "source": [
    "look_up_table = np.zeros((len(dictionary),message[\"message\"].to_numpy().shape[0]))\n",
    "for i in range(look_up_table.shape[0]):\n",
    "  for j in range(look_up_table.shape[1]):\n",
    "    if(dictionary[i] in message[\"message\"].to_numpy()[j]):\n",
    "      look_up_table[i][j] = list(message[\"message\"].to_numpy()[j]).count(dictionary[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_aYTzyfSkytu"
   },
   "source": [
    "## **Description of TF-IDF**\n",
    "\n",
    "TF-IDF stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. \n",
    "\n",
    "This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. \n",
    "\n",
    "Typically, the tf-idf weight is composed by two terms: \n",
    "1. Term Frequency (TF)\n",
    "2. Inverse Document Frequency (IDF)\n",
    "\n",
    "**Term Frequency (TF)**: Term Frequency is a measure of how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length as a way of normalization:\n",
    "\n",
    "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "\n",
    "**Inverse Document Frequency (IDF)**: Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:\n",
    "\n",
    "IDF(t) = log_e(Total number of documents / Number of documents with term t in it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-5iMSdlrO9RD"
   },
   "outputs": [],
   "source": [
    "idf = -np.log(np.count_nonzero(look_up_table, axis=1)/look_up_table.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LdxbluspQWb0"
   },
   "outputs": [],
   "source": [
    "tf = np.divide(look_up_table, np.sum(look_up_table, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFJMuwYGRL6H"
   },
   "outputs": [],
   "source": [
    "tf_idf = tf * idf[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vb_BpTlFf5DO"
   },
   "outputs": [],
   "source": [
    "tf_idf += sys.float_info.epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5otLVm9Zkou5"
   },
   "source": [
    "## **Prior Probability Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6XqcW0llj3FA"
   },
   "outputs": [],
   "source": [
    "p = float(message[message.labels == 'spam'].shape[0])\n",
    "p /= message[\"labels\"].shape[0]\n",
    "p = [1-p, p] # ham , spam p(c) -- prior probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rz5ODbZ1mT9L"
   },
   "source": [
    "## **Likelihood Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0W2zSgQEs-sE"
   },
   "outputs": [],
   "source": [
    "ham = np.array([])\n",
    "spam = np.array([])\n",
    "for i in range(label.size):\n",
    "  if(label[i] == \"ham\"):\n",
    "    if(ham.size == 0):\n",
    "      ham = np.array(tf_idf.T[i])\n",
    "    else:\n",
    "      ham = np.column_stack((ham,tf_idf.T[i]))\n",
    "  elif(label[i] == \"spam\"):\n",
    "    if(spam.size == 0):\n",
    "      spam = np.array(tf_idf.T[i])\n",
    "    else:\n",
    "      spam = np.column_stack((spam,tf_idf.T[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dlbKMNn3koY7"
   },
   "outputs": [],
   "source": [
    "ham_likelihood = np.mean(ham,axis=1)\n",
    "spam_likelihood = np.mean(spam,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CN7iYWW2uXYi"
   },
   "outputs": [],
   "source": [
    "likelihood = np.column_stack((ham_likelihood,spam_likelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfWo_2fumaiU"
   },
   "source": [
    "## **Multinomial Naive Bayes Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lAucSx9dGY5k",
    "outputId": "5c2296b8-41a2-4972-9e33-2495719d0dd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "pred = np.zeros(message[\"message\"].to_numpy().shape[0])\n",
    "for i in range(message[\"message\"].to_numpy().shape[0]):\n",
    "  posterior = np.log(p)\n",
    "  for j in range(len(message[\"message\"].to_numpy()[i])):\n",
    "    for k in range(len(p)):\n",
    "      posterior[k] +=  np.log(likelihood.T[k][dictionary.index(message[\"message\"].to_numpy()[i][j])])\n",
    "    pred[i] = np.argmax(posterior)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T9oZIATNmffN"
   },
   "source": [
    "## **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AuImh533wEB2"
   },
   "outputs": [],
   "source": [
    "ham_ham = 0 # true_pred\n",
    "ham_spam = 0 \n",
    "spam_ham = 0\n",
    "spam_spam = 0\n",
    "ctr_h = 0\n",
    "ctr_s = 0\n",
    "for i in range(prediction.size):\n",
    "  if(label[i] == \"ham\" and pred[i] == 0):\n",
    "    ctr_h += 1\n",
    "    ham_ham += 1\n",
    "  if(label[i] == \"ham\" and pred[i] == 1):\n",
    "    ctr_h += 1\n",
    "    ham_spam += 1\n",
    "  if(label[i] == \"spam\" and pred[i] == 0):\n",
    "    ctr_s += 1\n",
    "    spam_ham += 1\n",
    "  if(label[i] == \"spam\" and pred[i] == 1):\n",
    "    ctr_s += 1\n",
    "    spam_spam += 1\n",
    "\n",
    "confusion_matrix = np.array([[ham_ham/ctr_h , ham_spam/ctr_h],[spam_ham/ctr_s, spam_spam/ctr_s]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "OwDgeqtg5jFF",
    "outputId": "9ad25b2b-6050-4d65-f184-4d540ad3c6f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99585062e-01, 4.14937759e-04],\n",
       "       [1.33868809e-03, 9.98661312e-01]])"
      ]
     },
     "execution_count": 301,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-uCkVyynkVNj"
   },
   "source": [
    "# **Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "kl4BdFCW6NMc",
    "outputId": "ee6bca76-8ea3-4a8a-c660-6fdc8dca8c3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of ham training set: 11425 emails\n",
      "Size of spam training set: 11425 emails\n",
      "Percentage ham classified correctly: 99.95850622406638 %\n",
      "Percentage spam classified correctly: 99.8661311914324 %\n",
      "Total accuracy: 99.94611101131669 %\n",
      "False positives: 0.04149377593360996\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of ham training set: {} emails\".format(ham.shape[0]))\n",
    "print(\"Size of spam training set: {} emails\".format(spam.shape[0]))\n",
    "print(\"Percentage ham classified correctly: {} %\".format(confusion_matrix[0][0]*100))\n",
    "print(\"Percentage spam classified correctly: {} %\".format(confusion_matrix[1][1]*100))\n",
    "print(\"Total accuracy: {} %\".format(((ham_ham + spam_spam)/(ctr_h + ctr_s))*100))\n",
    "print(\"False positives: {}\".format(confusion_matrix[0][1]*100))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Question_3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
